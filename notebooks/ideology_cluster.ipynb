{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Mirko Draca and Carlo Schwarz selection of WVS questions and waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_wvs = pd.read_csv('../data/raw/wvs_ts_w1_w7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def recode_survey_responses(df, question_columns, neutral_values={3, 5}):\n",
    "    \"\"\"\n",
    "    Followin Draca & Schwarz (2024) methodology, this function recodes responses from the chosen waves (4-7) from the World Value Surve into\n",
    "    two indicator variables (support and oppose), imputing missing values, and calculating shares.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Survey DataFrame.\n",
    "    - question_columns (list): List of columns to transform.\n",
    "    - neutral_values (set): Values representing neutrality.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Transformed DataFrame with support/oppose indicators.\n",
    "    \"\"\"\n",
    "    new_df = df_wvs.rename(columns={\"COUNTRY_ALPHA\": \"country\", \"S020\": \"year\"}).copy()\n",
    "\n",
    "    # Impute missing values (negative values) with the sample mean of non-missing data\n",
    "    for col in question_columns:\n",
    "        # Impute missing values\n",
    "        valid_values = new_df[new_df[col] >= 0][col]  # Exclude negative values (missing data)\n",
    "        mean_value = valid_values.mean()\n",
    "        new_df[col] = new_df[col].apply(lambda x: mean_value if x < 0 else x)\n",
    "    \n",
    "    for col in question_columns:\n",
    "        # Recode based on specific column logic\n",
    "        if col == \"C002\":  # 1â€“3 scale (agree-disagree)\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x == 1 else 0)  # 1 means agree (support)\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x == 2 else 0)  # 2 means disagree (oppose)\n",
    "        elif col == \"G006\":  # 1â€“4 scale (1 and 2 = support, 3 and 4 = oppose)\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x in [3, 4] else 0)\n",
    "        elif col in [\"E036\", \"E037\", \"E039\"]:  # 1â€“10 scale\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x >= 6 else 0)  # 6-10 = support\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x <= 4 else 0)  # 1-4 = oppose\n",
    "        elif \"F1\" in col:  # 1â€“10 scale for F1... questions\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x >= 6 else 0)  # 6-10 = support\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x <= 4 else 0)  # 1-4 = oppose\n",
    "        else:  # Binary 0â€“1\n",
    "            new_df[f\"{col}_support\"] = new_df[col]\n",
    "            new_df[f\"{col}_oppose\"] = 1 - new_df[col]  # If it's binary, 1 - value gives the opposite\n",
    "\n",
    "    # Keep only relevant columns (support/oppose + country, year)\n",
    "    interest_columns = [\"country\", \"year\"] + [f\"{col}_support\" for col in question_columns] + [f\"{col}_oppose\" for col in question_columns]\n",
    "    new_df = new_df[interest_columns]\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "question_columns = [\"A124_02\", \"A124_06\", \"A124_07\", \"A124_08\", \"A124_09\", \n",
    "                    \"C002\", \"E036\", \"E037\", \"E039\", \"F114A\", \"F115\", \"F116\", \n",
    "                    \"F117\", \"F118\", \"F119\", \"F120\", \"F121\", \"F122\", \"F123\"]\n",
    "\n",
    "df_encoded = recode_survey_responses(df_wvs, question_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "0     ALB  1998              0.0              0.0              1.0   \n",
      "1     ALB  1998              0.0              0.0              1.0   \n",
      "2     ALB  1998              0.0              0.0              1.0   \n",
      "3     ALB  1998              0.0              0.0              1.0   \n",
      "4     ALB  1998              0.0              0.0              1.0   \n",
      "\n",
      "   A124_08_support  A124_09_support  C002_support  E036_support  E037_support  \\\n",
      "0              1.0              1.0             1             0             1   \n",
      "1              1.0              1.0             1             0             0   \n",
      "2              1.0              1.0             1             0             1   \n",
      "3              1.0              1.0             1             0             1   \n",
      "4              1.0              1.0             1             0             1   \n",
      "\n",
      "   ...  F114A_oppose  F115_oppose  F116_oppose  F117_oppose  F118_oppose  \\\n",
      "0  ...             0            1            1            1            1   \n",
      "1  ...             0            1            1            1            1   \n",
      "2  ...             0            1            1            1            1   \n",
      "3  ...             0            1            1            1            1   \n",
      "4  ...             0            1            1            1            1   \n",
      "\n",
      "   F119_oppose  F120_oppose  F121_oppose  F122_oppose  F123_oppose  \n",
      "0            1            0            0            1            1  \n",
      "1            1            0            0            1            1  \n",
      "2            1            0            0            1            1  \n",
      "3            1            1            1            1            1  \n",
      "4            1            1            1            1            1  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "## Exploring transformed file\n",
    "# Check the first few rows of the transformed DataFrame\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the data frame in the four waves\n",
    "\n",
    "# Wave 4 1999 - 2004\n",
    "df_wave4 = df_encoded[df_encoded['year'].between(1999, 2004)].copy()\n",
    "\n",
    "# Wave 5 2005 - 2009\n",
    "df_wave5 = df_encoded[df_encoded['year'].between(2005, 2009)].copy()\n",
    "\n",
    "# Wave 6 2010 - 2014\n",
    "df_wave6 = df_encoded[df_encoded['year'].between(2010, 2014)].copy()\n",
    "\n",
    "# Wave 7 2017 - 2022\n",
    "df_wave7 = df_encoded[df_encoded['year'].between(2017, 2022)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new DataFrame with the recoded responses\n",
    "grouped_df = recode_survey_responses(df_wvs, question_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "0     ALB  1998              0.0              0.0              1.0   \n",
      "1     ALB  1998              0.0              0.0              1.0   \n",
      "2     ALB  1998              0.0              0.0              1.0   \n",
      "3     ALB  1998              0.0              0.0              1.0   \n",
      "4     ALB  1998              0.0              0.0              1.0   \n",
      "\n",
      "   A124_08_support  A124_09_support  C002_support  E036_support  E037_support  \\\n",
      "0              1.0              1.0             1             0             1   \n",
      "1              1.0              1.0             1             0             0   \n",
      "2              1.0              1.0             1             0             1   \n",
      "3              1.0              1.0             1             0             1   \n",
      "4              1.0              1.0             1             0             1   \n",
      "\n",
      "   ...  F114A_oppose  F115_oppose  F116_oppose  F117_oppose  F118_oppose  \\\n",
      "0  ...             0            1            1            1            1   \n",
      "1  ...             0            1            1            1            1   \n",
      "2  ...             0            1            1            1            1   \n",
      "3  ...             0            1            1            1            1   \n",
      "4  ...             0            1            1            1            1   \n",
      "\n",
      "   F119_oppose  F120_oppose  F121_oppose  F122_oppose  F123_oppose  \n",
      "0            1            0            0            1            1  \n",
      "1            1            0            0            1            1  \n",
      "2            1            0            0            1            1  \n",
      "3            1            1            1            1            1  \n",
      "4            1            1            1            1            1  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dictionary from JSON file\n",
    "with open(\"variable_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    variable_dict = json.load(file)\n",
    "\n",
    "# Assuming df_encoded is the DataFrame with the survey data after recoding.\n",
    "# Let's inspect df_encoded for LDA\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Este sÃ­ jalÃ³\n",
    "\n",
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_encoded.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n",
    "#### Este sÃ­ jalÃ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ideology_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_5",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "996c5c1b-59a5-4915-95a4-541e86a2ab5a",
       "rows": [
        [
         "0",
         "A124_07_support",
         "F119_oppose",
         "F121_support",
         "F115_support",
         "A124_06_support"
        ],
        [
         "1",
         "A124_09_support",
         "F123_oppose",
         "A124_02_oppose",
         "F114A_support",
         "A124_02_support"
        ],
        [
         "2",
         "A124_08_support",
         "F120_oppose",
         "F117_oppose",
         "F116_support",
         "F119_oppose"
        ],
        [
         "3",
         "F118_oppose",
         "A124_02_oppose",
         "A124_06_oppose",
         "F121_support",
         "F123_oppose"
        ],
        [
         "4",
         "F119_oppose",
         "A124_06_oppose",
         "A124_09_oppose",
         "F120_support",
         "F117_oppose"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ideology_1</th>\n",
       "      <th>Ideology_2</th>\n",
       "      <th>Ideology_3</th>\n",
       "      <th>Ideology_4</th>\n",
       "      <th>Ideology_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A124_07_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F115_support</td>\n",
       "      <td>A124_06_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A124_09_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>F114A_support</td>\n",
       "      <td>A124_02_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A124_08_support</td>\n",
       "      <td>F120_oppose</td>\n",
       "      <td>F117_oppose</td>\n",
       "      <td>F116_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F118_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>A124_09_oppose</td>\n",
       "      <td>F120_support</td>\n",
       "      <td>F117_oppose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ideology_1      Ideology_2      Ideology_3     Ideology_4  \\\n",
       "0  A124_07_support     F119_oppose    F121_support   F115_support   \n",
       "1  A124_09_support     F123_oppose  A124_02_oppose  F114A_support   \n",
       "2  A124_08_support     F120_oppose     F117_oppose   F116_support   \n",
       "3      F118_oppose  A124_02_oppose  A124_06_oppose   F121_support   \n",
       "4      F119_oppose  A124_06_oppose  A124_09_oppose   F120_support   \n",
       "\n",
       "        Ideology_5  \n",
       "0  A124_06_support  \n",
       "1  A124_02_support  \n",
       "2      F119_oppose  \n",
       "3      F123_oppose  \n",
       "4      F117_oppose  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top Issues for Each Ideology\n",
    "top_issues = topic_words.apply(lambda x: x.nlargest(5).index.tolist(), axis=0)\n",
    "top_issues\n",
    "\n",
    "# Save the descriptive DataFrame to a CSV file\n",
    "top_issues.to_csv('top_issues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Folders for the results\n",
    "\n",
    "# Subfolders exist\n",
    "os.makedirs(\"../reports/lda_evaluation\", exist_ok=True)\n",
    "os.makedirs(\"../reports/top_issues\", exist_ok=True)\n",
    "\n",
    "# Example for saving evaluation results\n",
    "#eval_df.to_csv(f\"reports/lda_evaluation/wave{wave_number}_evaluation.csv\", index=False)\n",
    "\n",
    "# Later when saving top issues:\n",
    "#top_issues_df.to_csv(f\"reports/top_issues/wave{wave_number}_top_issues.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Setup\n",
    "num_folds = 10\n",
    "topic_range = range(1, 11)  # K = 1 to 10\n",
    "data_folder = \"data\"\n",
    "results_folder = \"../reports/model_evaluation\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "def compute_npmi(issue_list, test_data):\n",
    "    \"\"\"Compute NPMI for a list of top issues using test data.\"\"\"\n",
    "    N = len(test_data)\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    binary_data = test_data[issue_list].astype(int)\n",
    "    issue_counts = binary_data.sum(axis=0).to_dict()\n",
    "\n",
    "    npmi_scores = []\n",
    "    for i, j in combinations(issue_list, 2):\n",
    "        p_i = issue_counts[i] / N\n",
    "        p_j = issue_counts[j] / N\n",
    "        p_ij = ((binary_data[i] & binary_data[j]).sum()) / N\n",
    "\n",
    "        if p_ij > 0:\n",
    "            pmi = np.log(p_ij / (p_i * p_j + epsilon))\n",
    "            npmi = pmi / (-np.log(p_ij + epsilon))\n",
    "            npmi_scores.append(npmi)\n",
    "\n",
    "    return np.mean(npmi_scores) if npmi_scores else 0\n",
    "\n",
    "def evaluate_model_with_npmi(model, test_data, train_data):\n",
    "    \"\"\"Evaluate LDA model using average NPMI across all topics.\"\"\"\n",
    "    feature_names = train_data.columns\n",
    "    topic_words = pd.DataFrame(model.components_, columns=feature_names)\n",
    "    top_issues_per_topic = topic_words.apply(lambda x: x.nlargest(5).index.tolist(), axis=1)\n",
    "\n",
    "    topic_npmis = [compute_npmi(issue_list, test_data) for issue_list in top_issues_per_topic]\n",
    "    return np.mean(topic_npmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŠ Processing Wave 6\n",
      "\n",
      "ðŸŒŠ Processing Wave 5\n",
      "ðŸŒŠ Processing Wave 4\n",
      "\n",
      "\n",
      "ðŸŒŠ Processing Wave 7\n",
      "ðŸ“‚ Fold 1/10\n",
      "ðŸ“‚ Fold 1/10\n",
      "ðŸ“‚ Fold 1/10\n",
      "ðŸ“‚ Fold 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Saved evaluation results for Wave \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwave_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Run for waves 4â€“7 using the pre-split DataFrames\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_wave_from_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwave_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_wave4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_wave5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_wave6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_wave7\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ¯ Done! All waves processed using NPMI evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_wave_from_df(wave_number, df_wave):\n",
    "    print(f\"\\nðŸŒŠ Processing Wave {wave_number}\")\n",
    "    \n",
    "    lda_data = df_wave.drop(columns=[\"country\", \"year\"])\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize matrices to store results and track NPMI scores for each K\n",
    "    evaluation_matrix = np.zeros((num_folds, len(topic_range)))\n",
    "    npmi_scores_per_topic = {k: [] for k in topic_range}\n",
    "\n",
    "    # Fit models and evaluate each fold\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(lda_data)):\n",
    "        print(f\"ðŸ“‚ Fold {fold_idx+1}/{num_folds}\")\n",
    "        \n",
    "        train_data = lda_data.iloc[train_idx]\n",
    "        test_data = lda_data.iloc[test_idx]\n",
    "\n",
    "        for k_idx, n_topics in enumerate(topic_range):\n",
    "            lda = LatentDirichletAllocation(\n",
    "                n_components=n_topics,\n",
    "                doc_topic_prior=0.25,\n",
    "                topic_word_prior=0.1,\n",
    "                learning_method='online',\n",
    "                learning_decay=0.7,\n",
    "                learning_offset=10.0,\n",
    "                max_iter=20,\n",
    "                batch_size=1000,\n",
    "                evaluate_every=-1,\n",
    "                mean_change_tol=0.001,\n",
    "                max_doc_update_iter=100,\n",
    "                n_jobs=-1,\n",
    "                random_state=25\n",
    "            )\n",
    "            lda.fit(train_data)\n",
    "\n",
    "            # Evaluate with NPMI for the current fold and topic count\n",
    "            npmi_score = evaluate_model_with_npmi(lda, test_data, train_data)\n",
    "            evaluation_matrix[fold_idx, k_idx] = npmi_score\n",
    "\n",
    "            # Track NPMI scores for each topic count\n",
    "            npmi_scores_per_topic[n_topics].append(npmi_score)\n",
    "\n",
    "    # Calculate average NPMI for each topic count\n",
    "    avg_npmis = {k: np.mean(npmi_scores_per_topic[k]) for k in topic_range}\n",
    "\n",
    "    # Find the optimal number of topics (K) based on maximum average NPMI\n",
    "    optimal_k = max(avg_npmis, key=avg_npmis.get)\n",
    "    print(f\"Optimal number of topics for Wave {wave_number}: K={optimal_k}\")\n",
    "\n",
    "    # Save the evaluation matrix and the optimal K\n",
    "    eval_df = pd.DataFrame(evaluation_matrix, columns=[f\"K={k}\" for k in topic_range])\n",
    "    eval_df.to_csv(f\"{results_folder}/wave{wave_number}_evaluation.csv\", index=False)\n",
    "    print(f\"âœ… Saved evaluation results for Wave {wave_number}\")\n",
    "\n",
    "# Run for waves 4â€“7 using the pre-split DataFrames\n",
    "Parallel(n_jobs=4)(\n",
    "    delayed(process_wave_from_df)(wave_number, df)\n",
    "    for wave_number, df in zip(range(4, 8), [df_wave4, df_wave5, df_wave6, df_wave7])\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Done! All waves processed using NPMI evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
