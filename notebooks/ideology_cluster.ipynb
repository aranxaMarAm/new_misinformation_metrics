{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Mirko Draca and Carlo Schwarz selection of WVS questions and waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_wvs = pd.read_csv('../data/raw/wvs_ts_w1_w7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def recode_survey_responses(df, question_columns, neutral_values={3, 5}):\n",
    "    \"\"\"\n",
    "    Followin Draca & Schwarz (2024) methodology, this function recodes responses from the chosen waves (4-7) from the World Value Surve into\n",
    "    two indicator variables (support and oppose), imputing missing values, and calculating shares.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Survey DataFrame.\n",
    "    - question_columns (list): List of columns to transform.\n",
    "    - neutral_values (set): Values representing neutrality.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Transformed DataFrame with support/oppose indicators.\n",
    "    \"\"\"\n",
    "    new_df = df_wvs.rename(columns={\"COUNTRY_ALPHA\": \"country\", \"S020\": \"year\"}).copy()\n",
    "\n",
    "    # Impute missing values (negative values) with the sample mean of non-missing data\n",
    "    for col in question_columns:\n",
    "        # Impute missing values\n",
    "        valid_values = new_df[new_df[col] >= 0][col]  # Exclude negative values (missing data)\n",
    "        mean_value = valid_values.mean()\n",
    "        new_df[col] = new_df[col].apply(lambda x: mean_value if x < 0 else x)\n",
    "    \n",
    "    for col in question_columns:\n",
    "        # Recode based on specific column logic\n",
    "        if col == \"C002\":  # 1–3 scale (agree-disagree)\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x == 1 else 0)  # 1 means agree (support)\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x == 2 else 0)  # 2 means disagree (oppose)\n",
    "        elif col == \"G006\":  # 1–4 scale (1 and 2 = support, 3 and 4 = oppose)\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x in [3, 4] else 0)\n",
    "        elif col in [\"E036\", \"E037\", \"E039\"]:  # 1–10 scale\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x >= 6 else 0)  # 6-10 = support\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x <= 4 else 0)  # 1-4 = oppose\n",
    "        elif \"F1\" in col:  # 1–10 scale for F1... questions\n",
    "            new_df[f\"{col}_support\"] = new_df[col].apply(lambda x: 1 if x >= 6 else 0)  # 6-10 = support\n",
    "            new_df[f\"{col}_oppose\"] = new_df[col].apply(lambda x: 1 if x <= 4 else 0)  # 1-4 = oppose\n",
    "        else:  # Binary 0–1\n",
    "            new_df[f\"{col}_support\"] = new_df[col]\n",
    "            new_df[f\"{col}_oppose\"] = 1 - new_df[col]  # If it's binary, 1 - value gives the opposite\n",
    "\n",
    "    # Keep only relevant columns (support/oppose + country, year)\n",
    "    interest_columns = [\"country\", \"year\"] + [f\"{col}_support\" for col in question_columns] + [f\"{col}_oppose\" for col in question_columns]\n",
    "    new_df = new_df[interest_columns]\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "question_columns = [\"A124_02\", \"A124_06\", \"A124_07\", \"A124_08\", \"A124_09\", \n",
    "                    \"C002\", \"E036\", \"E037\", \"E039\", \"F114A\", \"F115\", \"F116\", \n",
    "                    \"F117\", \"F118\", \"F119\", \"F120\", \"F121\", \"F122\", \"F123\"]\n",
    "\n",
    "df_encoded = recode_survey_responses(df_wvs, question_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "0     ALB  1998              0.0              0.0              1.0   \n",
      "1     ALB  1998              0.0              0.0              1.0   \n",
      "2     ALB  1998              0.0              0.0              1.0   \n",
      "3     ALB  1998              0.0              0.0              1.0   \n",
      "4     ALB  1998              0.0              0.0              1.0   \n",
      "\n",
      "   A124_08_support  A124_09_support  C002_support  E036_support  E037_support  \\\n",
      "0              1.0              1.0             1             0             1   \n",
      "1              1.0              1.0             1             0             0   \n",
      "2              1.0              1.0             1             0             1   \n",
      "3              1.0              1.0             1             0             1   \n",
      "4              1.0              1.0             1             0             1   \n",
      "\n",
      "   ...  F114A_oppose  F115_oppose  F116_oppose  F117_oppose  F118_oppose  \\\n",
      "0  ...             0            1            1            1            1   \n",
      "1  ...             0            1            1            1            1   \n",
      "2  ...             0            1            1            1            1   \n",
      "3  ...             0            1            1            1            1   \n",
      "4  ...             0            1            1            1            1   \n",
      "\n",
      "   F119_oppose  F120_oppose  F121_oppose  F122_oppose  F123_oppose  \n",
      "0            1            0            0            1            1  \n",
      "1            1            0            0            1            1  \n",
      "2            1            0            0            1            1  \n",
      "3            1            1            1            1            1  \n",
      "4            1            1            1            1            1  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "## Exploring transformed file\n",
    "# Check the first few rows of the transformed DataFrame\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the data frame in the four waves\n",
    "\n",
    "# Wave 4 1999 - 2004\n",
    "df_wave4 = df_encoded[df_encoded['year'].between(1999, 2004)].copy()\n",
    "\n",
    "# Wave 5 2005 - 2009\n",
    "df_wave5 = df_encoded[df_encoded['year'].between(2005, 2009)].copy()\n",
    "\n",
    "# Wave 6 2010 - 2014\n",
    "df_wave6 = df_encoded[df_encoded['year'].between(2010, 2014)].copy()\n",
    "\n",
    "# Wave 7 2017 - 2022\n",
    "df_wave7 = df_encoded[df_encoded['year'].between(2017, 2022)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new DataFrame with the recoded responses\n",
    "grouped_df = recode_survey_responses(df_wvs, question_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "0     ALB  1998              0.0              0.0              1.0   \n",
      "1     ALB  1998              0.0              0.0              1.0   \n",
      "2     ALB  1998              0.0              0.0              1.0   \n",
      "3     ALB  1998              0.0              0.0              1.0   \n",
      "4     ALB  1998              0.0              0.0              1.0   \n",
      "\n",
      "   A124_08_support  A124_09_support  C002_support  E036_support  E037_support  \\\n",
      "0              1.0              1.0             1             0             1   \n",
      "1              1.0              1.0             1             0             0   \n",
      "2              1.0              1.0             1             0             1   \n",
      "3              1.0              1.0             1             0             1   \n",
      "4              1.0              1.0             1             0             1   \n",
      "\n",
      "   ...  F114A_oppose  F115_oppose  F116_oppose  F117_oppose  F118_oppose  \\\n",
      "0  ...             0            1            1            1            1   \n",
      "1  ...             0            1            1            1            1   \n",
      "2  ...             0            1            1            1            1   \n",
      "3  ...             0            1            1            1            1   \n",
      "4  ...             0            1            1            1            1   \n",
      "\n",
      "   F119_oppose  F120_oppose  F121_oppose  F122_oppose  F123_oppose  \n",
      "0            1            0            0            1            1  \n",
      "1            1            0            0            1            1  \n",
      "2            1            0            0            1            1  \n",
      "3            1            1            1            1            1  \n",
      "4            1            1            1            1            1  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dictionary from JSON file\n",
    "with open(\"variable_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    variable_dict = json.load(file)\n",
    "\n",
    "# Assuming df_encoded is the DataFrame with the survey data after recoding.\n",
    "# Let's inspect df_encoded for LDA\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_encoded.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wave4...\n",
      "wave4 LDA results saved!\n",
      "Processing wave5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     26\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(\n\u001b[1;32m     27\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mnum_topics,\n\u001b[1;32m     28\u001b[0m     doc_topic_prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Fit LDA model and transform data\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m lda_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Extract Topic-Feature Importance\u001b[39;00m\n\u001b[1;32m     46\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m lda_data\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:778\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit_transform\u001b[0;34m(self, X, y, normalize)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    756\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m    Fit to data, then transform it.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m        Transformed array.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X, normalize\u001b[38;5;241m=\u001b[39mnormalize)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:704\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# calculate final perplexity value on train set\u001b[39;00m\n\u001b[1;32m    701\u001b[0m doc_topics_distr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(\n\u001b[1;32m    702\u001b[0m     X, cal_sstats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, parallel\u001b[38;5;241m=\u001b[39mparallel\n\u001b[1;32m    703\u001b[0m )\n\u001b[0;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perplexity_precomp_distr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topics_distr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    706\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:918\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._perplexity_precomp_distr\u001b[0;34m(self, X, doc_topic_distr, sub_sampling)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of topics does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    917\u001b[0m current_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 918\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_approx_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_distr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sub_sampling:\n\u001b[1;32m    921\u001b[0m     word_cnt \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_samples) \u001b[38;5;241m/\u001b[39m current_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:839\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._approx_bound\u001b[0;34m(self, X, doc_topic_distr, sub_sampling)\u001b[0m\n\u001b[1;32m    835\u001b[0m         cnts \u001b[38;5;241m=\u001b[39m X[idx_d, ids]\n\u001b[1;32m    836\u001b[0m     temp \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    837\u001b[0m         dirichlet_doc_topic[idx_d, :, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m dirichlet_component_[:, ids]\n\u001b[1;32m    838\u001b[0m     )\n\u001b[0;32m--> 839\u001b[0m     norm_phi \u001b[38;5;241m=\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts, norm_phi)\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# compute E[log p(theta | alpha) - log q(theta | gamma)]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/scipy/special/_logsumexp.py:118\u001b[0m, in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp_size(a) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# log of zero is OK\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m         out, sgn \u001b[38;5;241m=\u001b[39m \u001b[43m_logsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# NumPy is convenient for shape manipulation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/scipy/special/_logsumexp.py:202\u001b[0m, in \u001b[0;36m_logsumexp\u001b[0;34m(a, b, axis, return_sign, xp)\u001b[0m\n\u001b[1;32m    198\u001b[0m     a[b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mxp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Find element with maximum real part, since this is what affects the magnitude\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# of the exponential. Possible enhancement: include log of `b` magnitude in `a`.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m a_max, i_max \u001b[38;5;241m=\u001b[39m \u001b[43m_elements_and_indices_with_max_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# for precision, these terms are separated out of the main sum.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m a[i_max] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mxp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/scipy/special/_logsumexp.py:182\u001b[0m, in \u001b[0;36m_elements_and_indices_with_max_real\u001b[0;34m(a, axis, xp)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39msum(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mdtype, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     mask \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mmax\u001b[39m), xp\u001b[38;5;241m.\u001b[39masarray(mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3199\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3080\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   3083\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   3086\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Define the number of topics for LDA\n",
    "num_topics = 10\n",
    "\n",
    "# Dictionary of waves (Modify these variables to match your actual dataframes)\n",
    "df_waves = {\n",
    "    \"wave4\": df_wave4,\n",
    "    \"wave5\": df_wave5,\n",
    "    \"wave6\": df_wave6,\n",
    "    \"wave7\": df_wave7\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "lda_results = {}\n",
    "\n",
    "# Loop through each wave\n",
    "for wave, df in df_waves.items():\n",
    "    print(f\"Processing {wave}...\")\n",
    "\n",
    "    # Drop 'country' and 'year' for LDA\n",
    "    lda_data = df.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "    # Initialize LDA model\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        doc_topic_prior=0.25,\n",
    "        topic_word_prior=0.1,\n",
    "        learning_method='online',\n",
    "        learning_decay=0.7,\n",
    "        learning_offset=10.0,\n",
    "        max_iter=50,\n",
    "        batch_size=1000,\n",
    "        evaluate_every=-1,\n",
    "        mean_change_tol=0.001,\n",
    "        max_doc_update_iter=300,\n",
    "        n_jobs=-1,\n",
    "        random_state=25\n",
    "    )\n",
    "\n",
    "    # Fit LDA model and transform data\n",
    "    lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "    # Extract Topic-Feature Importance\n",
    "    feature_names = lda_data.columns\n",
    "    topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "    # Normalize importance scores\n",
    "    topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "    topic_words = topic_words.T  # Transpose for readability\n",
    "    topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Store results in dictionary\n",
    "    lda_results[wave] = topic_words\n",
    "\n",
    "    # Save results as CSV\n",
    "    topic_words.to_csv(f\"../reports/lda_results_{wave}.csv\")\n",
    "\n",
    "    print(f\"{wave} LDA results saved!\")\n",
    "\n",
    "print(\"✅ All waves processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_wave4.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups (you can tweak num_topics as needed)\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_wave5.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups (you can tweak num_topics as needed)\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_wave6.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups (you can tweak num_topics as needed)\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_wave7.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups (you can tweak num_topics as needed)\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ideology_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_5",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "996c5c1b-59a5-4915-95a4-541e86a2ab5a",
       "rows": [
        [
         "0",
         "A124_07_support",
         "F119_oppose",
         "F121_support",
         "F115_support",
         "A124_06_support"
        ],
        [
         "1",
         "A124_09_support",
         "F123_oppose",
         "A124_02_oppose",
         "F114A_support",
         "A124_02_support"
        ],
        [
         "2",
         "A124_08_support",
         "F120_oppose",
         "F117_oppose",
         "F116_support",
         "F119_oppose"
        ],
        [
         "3",
         "F118_oppose",
         "A124_02_oppose",
         "A124_06_oppose",
         "F121_support",
         "F123_oppose"
        ],
        [
         "4",
         "F119_oppose",
         "A124_06_oppose",
         "A124_09_oppose",
         "F120_support",
         "F117_oppose"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ideology_1</th>\n",
       "      <th>Ideology_2</th>\n",
       "      <th>Ideology_3</th>\n",
       "      <th>Ideology_4</th>\n",
       "      <th>Ideology_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A124_07_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F115_support</td>\n",
       "      <td>A124_06_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A124_09_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>F114A_support</td>\n",
       "      <td>A124_02_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A124_08_support</td>\n",
       "      <td>F120_oppose</td>\n",
       "      <td>F117_oppose</td>\n",
       "      <td>F116_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F118_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>A124_09_oppose</td>\n",
       "      <td>F120_support</td>\n",
       "      <td>F117_oppose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ideology_1      Ideology_2      Ideology_3     Ideology_4  \\\n",
       "0  A124_07_support     F119_oppose    F121_support   F115_support   \n",
       "1  A124_09_support     F123_oppose  A124_02_oppose  F114A_support   \n",
       "2  A124_08_support     F120_oppose     F117_oppose   F116_support   \n",
       "3      F118_oppose  A124_02_oppose  A124_06_oppose   F121_support   \n",
       "4      F119_oppose  A124_06_oppose  A124_09_oppose   F120_support   \n",
       "\n",
       "        Ideology_5  \n",
       "0  A124_06_support  \n",
       "1  A124_02_support  \n",
       "2      F119_oppose  \n",
       "3      F123_oppose  \n",
       "4      F117_oppose  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top Issues for Each Ideology\n",
    "top_issues = topic_words.apply(lambda x: x.nlargest(5).index.tolist(), axis=0)\n",
    "top_issues\n",
    "\n",
    "# Save the descriptive DataFrame to a CSV file\n",
    "top_issues.to_csv('top_issues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Ideology_1  \\\n",
      "0  People with AIDS as neighbors_support   \n",
      "1       Homosexuals as neighbors_support   \n",
      "2      Drug addicts as neighbors_support   \n",
      "3    Homosexuality – justifiable?_oppose   \n",
      "4     Prostitution – justifiable?_oppose   \n",
      "\n",
      "                                       Ideology_2  \\\n",
      "0              Prostitution – justifiable?_oppose   \n",
      "1                   Suicide – justifiable?_oppose   \n",
      "2                  Abortion – justifiable?_oppose   \n",
      "3              Different race as neighbors_oppose   \n",
      "4  Immigrants foreign workers as neighbors_oppose   \n",
      "\n",
      "                                        Ideology_3  \\\n",
      "0                   Divorce – justifiable?_support   \n",
      "1               Different race as neighbors_oppose   \n",
      "2  Someone accepting a bribe – justifiable?_oppose   \n",
      "3   Immigrants foreign workers as neighbors_oppose   \n",
      "4                  Homosexuals as neighbors_oppose   \n",
      "\n",
      "                                          Ideology_4  \\\n",
      "0  Avoiding a fare on public transport – justifia...   \n",
      "1  Justifiable: Claiming government benefits to w...   \n",
      "2           Cheating on taxes – justifiable?_support   \n",
      "3                     Divorce – justifiable?_support   \n",
      "4                    Abortion – justifiable?_support   \n",
      "\n",
      "                                        Ideology_5  \n",
      "0  Immigrants foreign workers as neighbors_support  \n",
      "1              Different race as neighbors_support  \n",
      "2               Prostitution – justifiable?_oppose  \n",
      "3                    Suicide – justifiable?_oppose  \n",
      "4  Someone accepting a bribe – justifiable?_oppose  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the dictionary from the JSON file\n",
    "with open('variable_dict.json', 'r') as f:\n",
    "    variable_dict = json.load(f)\n",
    "\n",
    "# Read the CSV file with the top issues\n",
    "top_issues = pd.read_csv('top_issues.csv')\n",
    "\n",
    "# Replace codes with their dictionary description (keeps the '_support' or '_oppose' component)\n",
    "def replace_with_description(issue_codes):\n",
    "    # Split by last underscore to separate the base code from the suffix\n",
    "    return [f\"{variable_dict.get(code.rsplit('_', 1)[0], f'Description Not Found: {code}')}{'_' + code.split('_')[-1]}\" for code in issue_codes]\n",
    "\n",
    "# Apply the function to each row of the 'top_issues' DataFrame\n",
    "top_issues_descriptive = top_issues.apply(lambda row: replace_with_description(row), axis=0)\n",
    "\n",
    "# Display the DataFrame with descriptive labels\n",
    "print(top_issues_descriptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your encoded dataset (df_encoded should be numeric)\n",
    "df_encoded = pd.read_csv(\"your_encoded_data.csv\")\n",
    "\n",
    "# Define the number of ideological types (topics) to test\n",
    "n_components_range = range(1, 11)  # Testing from 1 to 10 ideological types\n",
    "k_folds = 10  # Number of folds\n",
    "\n",
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to compute topic cohesion\n",
    "def topic_cohesion_score(topic_distributions, df_original):\n",
    "    \"\"\"\n",
    "    Measures how often top issue positions appear together in the original dataset.\n",
    "    Higher values indicate better cohesion.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for topic in topic_distributions:\n",
    "        # Get the top issues for the topic\n",
    "        top_issues = topic.argsort()[-5:]  # Top 5 most important features\n",
    "        co_occurrence = df_original.iloc[:, top_issues].mean().mean()  # Average co-occurrence\n",
    "        scores.append(co_occurrence)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Store the results\n",
    "best_cohesion = -np.inf\n",
    "best_n_components = None\n",
    "best_lda_model = None\n",
    "\n",
    "# Iterate over different numbers of ideological types\n",
    "for n_components in n_components_range:\n",
    "    cohesion_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(df_encoded):\n",
    "        train_data, test_data = df_encoded.iloc[train_index], df_encoded.iloc[test_index]\n",
    "\n",
    "        # Scale the data (LDA performs better with standardized inputs)\n",
    "        scaler = StandardScaler()\n",
    "        train_data_scaled = scaler.fit_transform(train_data)\n",
    "        test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "        # Train LDA model\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=n_components, \n",
    "            learning_method='batch',  # Use batch for stability\n",
    "            max_iter=10, \n",
    "            random_state=42\n",
    "        )\n",
    "        lda.fit(train_data_scaled)\n",
    "        \n",
    "        # Get topic distributions for the test set\n",
    "        test_topic_distributions = lda.transform(test_data_scaled)\n",
    "\n",
    "        # Compute topic cohesion\n",
    "        cohesion = topic_cohesion_score(lda.components_, df_encoded)\n",
    "        cohesion_scores.append(cohesion)\n",
    "\n",
    "    # Average cohesion score across all folds\n",
    "    avg_cohesion = np.mean(cohesion_scores)\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_cohesion > best_cohesion:\n",
    "        best_cohesion = avg_cohesion\n",
    "        best_n_components = n_components\n",
    "        best_lda_model = lda\n",
    "\n",
    "# Print the best model parameters\n",
    "print(f\"Best Number of Ideological Types: {best_n_components}\")\n",
    "print(f\"Best Topic Cohesion Score: {best_cohesion}\")\n",
    "\n",
    "# Save the best LDA model\n",
    "import joblib\n",
    "joblib.dump(best_lda_model, \"best_lda_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try this next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# **SETTINGS**\n",
    "num_topics = 10\n",
    "num_folds = 10\n",
    "data_folder = \"data\"\n",
    "reports_folder = \"reports\"\n",
    "\n",
    "# Ensure reports folder exists\n",
    "os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "# Load the dictionary for variable descriptions\n",
    "with open(os.path.join(data_folder, 'variable_dic.json'), 'r') as f:\n",
    "    variable_dict = json.load(f)\n",
    "\n",
    "# Function to process each wave\n",
    "def process_wave(wave_number):\n",
    "    # Load wave data\n",
    "    file_path = os.path.join(data_folder, f\"wave{wave_number}_data.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop country and year\n",
    "    data = df.drop(columns=[\"country\", \"year\"])\n",
    "    \n",
    "    # 10-Fold Cross-Validation (Train on First Fold)\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=25)\n",
    "    for train_idx, test_idx in kf.split(data):\n",
    "        train_data, test_data = data.iloc[train_idx], data.iloc[test_idx]\n",
    "        break  # Use only first fold for training\n",
    "\n",
    "    # Train LDA Model\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        doc_topic_prior=0.25,\n",
    "        topic_word_prior=0.1,\n",
    "        learning_method='online',\n",
    "        learning_decay=0.7,\n",
    "        learning_offset=10.0,\n",
    "        max_iter=50,\n",
    "        batch_size=1000,\n",
    "        evaluate_every=-1,\n",
    "        mean_change_tol=0.001,\n",
    "        max_doc_update_iter=300,\n",
    "        n_jobs=-1,\n",
    "        random_state=25\n",
    "    )\n",
    "    lda_matrix = lda_model.fit_transform(train_data)\n",
    "\n",
    "    # Extract topic-feature importance\n",
    "    feature_names = data.columns\n",
    "    topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "    # Normalize Importance Scores\n",
    "    topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "    topic_words = topic_words.T  # Transpose for better visualization\n",
    "    topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n",
    "    # Extract Top 10 Issues per Ideology\n",
    "    top_issues = topic_words.apply(lambda x: x.nlargest(10).index.tolist(), axis=0)\n",
    "\n",
    "    # Replace issue codes with descriptions\n",
    "    def replace_with_description(issue_codes):\n",
    "        return [variable_dict.get(code.split('_')[0], code) for code in issue_codes]\n",
    "\n",
    "    top_issues_descriptive = top_issues.apply(replace_with_description, axis=0)\n",
    "\n",
    "    # Save to reports folder\n",
    "    output_file = os.path.join(reports_folder, f\"top_issues_wave{wave_number}.csv\")\n",
    "    top_issues_descriptive.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Saved: {output_file}\")\n",
    "\n",
    "# **Run for waves 4 through 7**\n",
    "for wave in range(4, 8):\n",
    "    process_wave(wave)\n",
    "\n",
    "print(\"🎉 All waves processed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
