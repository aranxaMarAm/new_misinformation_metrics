{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Mirko Draca and Carlo Schwarz selection of WVS questions and waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_wvs = pd.read_csv('../data/raw/wvs_ts_w1_w7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_survey_responses(df_wvs, question_columns, neutral_values={3, 5}):\n",
    "    \"\"\"\n",
    "    Following Draca & Schwarz (2024) methodology, this function recodes responses from WVS waves 4-7 \n",
    "    into support and oppose indicators, imputes missing values, and filters countries based on response completeness.\n",
    "\n",
    "    Parameters:\n",
    "    - df_wvs (pd.DataFrame): Survey DataFrame.\n",
    "    - question_columns (list): List of columns to transform.\n",
    "    - neutral_values (set): Values representing neutrality (not used here but can be extended).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Transformed DataFrame with support/oppose indicators and country/year.\n",
    "    \"\"\"\n",
    "    # Step 1: Rename and filter for years >= 1999\n",
    "    new_df = df_wvs.rename(columns={\"COUNTRY_ALPHA\": \"country\", \"S020\": \"year\", \"S002VS\": \"wave\"}) \\\n",
    "                   .loc[lambda df: df[\"year\"] >= 1999] \\\n",
    "                   .copy()\n",
    "\n",
    "    # Step 2: Filter for relevant waves\n",
    "    valid_waves = [4, 5, 6, 7]\n",
    "    df_filtered = new_df[new_df[\"wave\"].isin(valid_waves)].copy()\n",
    "\n",
    "    # Step 3: Find countries that appear in at least 3 of the 4 waves\n",
    "    wave_counts = df_filtered.groupby(\"country\")[\"wave\"].nunique()\n",
    "    eligible_countries = wave_counts[wave_counts >= 3].index\n",
    "    df_final = df_filtered[df_filtered[\"country\"].isin(eligible_countries)].copy()\n",
    "\n",
    "    # Step 4: Impute missing values for each question column\n",
    "    for col in question_columns:\n",
    "        valid_values = df_final[df_final[col] >= 0][col]\n",
    "        mean_value = valid_values.mean()\n",
    "        df_final[col] = df_final[col].apply(lambda x: mean_value if x < 0 else x)\n",
    "\n",
    "    # Step 5: Recode into support and oppose\n",
    "    for col in question_columns:\n",
    "        if col == \"C002\":  # 1–3 scale (agree-disagree)\n",
    "            df_final[f\"{col}_support\"] = df_final[col].apply(lambda x: 1 if x == 1 else 0)\n",
    "            df_final[f\"{col}_oppose\"] = df_final[col].apply(lambda x: 1 if x == 2 else 0)\n",
    "        elif col in [\"G006\", \"E069_01\", \"E069_02\", \"E069_04\", \"E069_05\", \"E069_06\",\n",
    "             \"E069_07\", \"E069_08\", \"E069_13\", \"E069_17\"]:  # 1–4 scale\n",
    "             df_final[f\"{col}_support\"] = df_final[col].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "             df_final[f\"{col}_oppose\"] = df_final[col].apply(lambda x: 1 if x in [3, 4] else 0)\n",
    "        elif col in [\"E036\", \"E037\", \"E039\"] or \"F1\" in col:  # 1–10 scale\n",
    "            df_final[f\"{col}_support\"] = df_final[col].apply(lambda x: 1 if x >= 6 else 0)\n",
    "            df_final[f\"{col}_oppose\"] = df_final[col].apply(lambda x: 1 if x <= 4 else 0)\n",
    "        else:  # Binary 0–1\n",
    "            df_final[f\"{col}_support\"] = df_final[col]\n",
    "            df_final[f\"{col}_oppose\"] = 1 - df_final[col]\n",
    "\n",
    "    # Step 6: Keep only relevant columns\n",
    "    interest_columns = [\"country\", \"year\"] + \\\n",
    "                       [f\"{col}_support\" for col in question_columns] + \\\n",
    "                       [f\"{col}_oppose\" for col in question_columns]\n",
    "\n",
    "    return df_final[interest_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "question_columns = [\n",
    "    \"A124_02\", \"A124_06\", \"A124_07\", \"A124_08\", \"A124_09\", \"C002\", \"E036\", \"E037\", \"E039\",\n",
    "    \"F114A\", \"F115\", \"F116\", \"F117\", \"F118\", \"F119\", \"F120\", \"F121\", \"F122\", \"F123\", \"G006\", \n",
    "    \"E069_01\", \"E069_02\", \"E069_04\", \"E069_05\", \"E069_06\", \"E069_07\", \"E069_08\", \"E069_13\", \"E069_17\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/c6csf7ws6pj9wy406twfjkfw0000gn/T/ipykernel_31029/4208553163.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final[f\"{col}_support\"] = df_final[col].apply(lambda x: 1 if x in [1, 2] else 0)\n",
      "/var/folders/vk/c6csf7ws6pj9wy406twfjkfw0000gn/T/ipykernel_31029/4208553163.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final[f\"{col}_oppose\"] = df_final[col].apply(lambda x: 1 if x in [3, 4] else 0)\n"
     ]
    }
   ],
   "source": [
    "df_encoded = recode_survey_responses(df_wvs, question_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "7092     ARG  1999              0.0              0.0              0.0   \n",
      "7093     ARG  1999              0.0              0.0              0.0   \n",
      "7094     ARG  1999              0.0              0.0              1.0   \n",
      "7095     ARG  1999              0.0              0.0              0.0   \n",
      "7096     ARG  1999              0.0              0.0              0.0   \n",
      "\n",
      "      A124_08_support  A124_09_support  C002_support  E036_support  \\\n",
      "7092              0.0              0.0             0             0   \n",
      "7093              1.0              0.0             1             1   \n",
      "7094              1.0              1.0             1             1   \n",
      "7095              0.0              0.0             0             0   \n",
      "7096              0.0              0.0             1             0   \n",
      "\n",
      "      E037_support  ...  G006_oppose  E069_01_oppose  E069_02_oppose  \\\n",
      "7092             1  ...            0               0               1   \n",
      "7093             0  ...            0               0               1   \n",
      "7094             0  ...            0               0               1   \n",
      "7095             0  ...            0               1               1   \n",
      "7096             1  ...            0               1               1   \n",
      "\n",
      "      E069_04_oppose  E069_05_oppose  E069_06_oppose  E069_07_oppose  \\\n",
      "7092               1               1               1               1   \n",
      "7093               1               1               1               1   \n",
      "7094               1               1               1               1   \n",
      "7095               1               1               0               1   \n",
      "7096               0               1               1               1   \n",
      "\n",
      "      E069_08_oppose  E069_13_oppose  E069_17_oppose  \n",
      "7092               1               1               0  \n",
      "7093               1               1               0  \n",
      "7094               1               1               0  \n",
      "7095               1               0               0  \n",
      "7096               1               0               0  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "## Exploring transformed file\n",
    "# Check the first few rows of the transformed DataFrame\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the data frame in the four waves\n",
    "\n",
    "# Wave 4 1999 - 2004\n",
    "df_wave4 = df_encoded[df_encoded['year'].between(1999, 2004)].copy()\n",
    "\n",
    "# Wave 5 2005 - 2009\n",
    "df_wave5 = df_encoded[df_encoded['year'].between(2005, 2009)].copy()\n",
    "\n",
    "# Wave 6 2010 - 2014\n",
    "df_wave6 = df_encoded[df_encoded['year'].between(2010, 2014)].copy()\n",
    "\n",
    "# Wave 7 2017 - 2022\n",
    "df_wave7 = df_encoded[df_encoded['year'].between(2017, 2022)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 4 dimensions: (46100, 60)\n",
      "Wave 5 dimensions: (57982, 60)\n",
      "Wave 6 dimensions: (60783, 60)\n",
      "Wave 7 dimensions: (62706, 60)\n"
     ]
    }
   ],
   "source": [
    "## Function to print dimensions of all the frames (wave 4-7)\n",
    "for i, df in enumerate([df_wave4, df_wave5, df_wave6, df_wave7], start=4):\n",
    "    print(f\"Wave {i} dimensions: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names of the transformed DataFrames:\n",
      "Wave 4 columns: ['country', 'year', 'A124_02_support', 'A124_06_support', 'A124_07_support', 'A124_08_support', 'A124_09_support', 'C002_support', 'E036_support', 'E037_support', 'E039_support', 'F114A_support', 'F115_support', 'F116_support', 'F117_support', 'F118_support', 'F119_support', 'F120_support', 'F121_support', 'F122_support', 'F123_support', 'G006_support', 'E069_01_support', 'E069_02_support', 'E069_04_support', 'E069_05_support', 'E069_06_support', 'E069_07_support', 'E069_08_support', 'E069_13_support', 'E069_17_support', 'A124_02_oppose', 'A124_06_oppose', 'A124_07_oppose', 'A124_08_oppose', 'A124_09_oppose', 'C002_oppose', 'E036_oppose', 'E037_oppose', 'E039_oppose', 'F114A_oppose', 'F115_oppose', 'F116_oppose', 'F117_oppose', 'F118_oppose', 'F119_oppose', 'F120_oppose', 'F121_oppose', 'F122_oppose', 'F123_oppose', 'G006_oppose', 'E069_01_oppose', 'E069_02_oppose', 'E069_04_oppose', 'E069_05_oppose', 'E069_06_oppose', 'E069_07_oppose', 'E069_08_oppose', 'E069_13_oppose', 'E069_17_oppose']\n",
      "Wave 5 columns: ['country', 'year', 'A124_02_support', 'A124_06_support', 'A124_07_support', 'A124_08_support', 'A124_09_support', 'C002_support', 'E036_support', 'E037_support', 'E039_support', 'F114A_support', 'F115_support', 'F116_support', 'F117_support', 'F118_support', 'F119_support', 'F120_support', 'F121_support', 'F122_support', 'F123_support', 'G006_support', 'E069_01_support', 'E069_02_support', 'E069_04_support', 'E069_05_support', 'E069_06_support', 'E069_07_support', 'E069_08_support', 'E069_13_support', 'E069_17_support', 'A124_02_oppose', 'A124_06_oppose', 'A124_07_oppose', 'A124_08_oppose', 'A124_09_oppose', 'C002_oppose', 'E036_oppose', 'E037_oppose', 'E039_oppose', 'F114A_oppose', 'F115_oppose', 'F116_oppose', 'F117_oppose', 'F118_oppose', 'F119_oppose', 'F120_oppose', 'F121_oppose', 'F122_oppose', 'F123_oppose', 'G006_oppose', 'E069_01_oppose', 'E069_02_oppose', 'E069_04_oppose', 'E069_05_oppose', 'E069_06_oppose', 'E069_07_oppose', 'E069_08_oppose', 'E069_13_oppose', 'E069_17_oppose']\n",
      "Wave 6 columns: ['country', 'year', 'A124_02_support', 'A124_06_support', 'A124_07_support', 'A124_08_support', 'A124_09_support', 'C002_support', 'E036_support', 'E037_support', 'E039_support', 'F114A_support', 'F115_support', 'F116_support', 'F117_support', 'F118_support', 'F119_support', 'F120_support', 'F121_support', 'F122_support', 'F123_support', 'G006_support', 'E069_01_support', 'E069_02_support', 'E069_04_support', 'E069_05_support', 'E069_06_support', 'E069_07_support', 'E069_08_support', 'E069_13_support', 'E069_17_support', 'A124_02_oppose', 'A124_06_oppose', 'A124_07_oppose', 'A124_08_oppose', 'A124_09_oppose', 'C002_oppose', 'E036_oppose', 'E037_oppose', 'E039_oppose', 'F114A_oppose', 'F115_oppose', 'F116_oppose', 'F117_oppose', 'F118_oppose', 'F119_oppose', 'F120_oppose', 'F121_oppose', 'F122_oppose', 'F123_oppose', 'G006_oppose', 'E069_01_oppose', 'E069_02_oppose', 'E069_04_oppose', 'E069_05_oppose', 'E069_06_oppose', 'E069_07_oppose', 'E069_08_oppose', 'E069_13_oppose', 'E069_17_oppose']\n",
      "Wave 7 columns: ['country', 'year', 'A124_02_support', 'A124_06_support', 'A124_07_support', 'A124_08_support', 'A124_09_support', 'C002_support', 'E036_support', 'E037_support', 'E039_support', 'F114A_support', 'F115_support', 'F116_support', 'F117_support', 'F118_support', 'F119_support', 'F120_support', 'F121_support', 'F122_support', 'F123_support', 'G006_support', 'E069_01_support', 'E069_02_support', 'E069_04_support', 'E069_05_support', 'E069_06_support', 'E069_07_support', 'E069_08_support', 'E069_13_support', 'E069_17_support', 'A124_02_oppose', 'A124_06_oppose', 'A124_07_oppose', 'A124_08_oppose', 'A124_09_oppose', 'C002_oppose', 'E036_oppose', 'E037_oppose', 'E039_oppose', 'F114A_oppose', 'F115_oppose', 'F116_oppose', 'F117_oppose', 'F118_oppose', 'F119_oppose', 'F120_oppose', 'F121_oppose', 'F122_oppose', 'F123_oppose', 'G006_oppose', 'E069_01_oppose', 'E069_02_oppose', 'E069_04_oppose', 'E069_05_oppose', 'E069_06_oppose', 'E069_07_oppose', 'E069_08_oppose', 'E069_13_oppose', 'E069_17_oppose']\n"
     ]
    }
   ],
   "source": [
    "# Column names of the transformed DataFrames\n",
    "print(\"Column names of the transformed DataFrames:\")\n",
    "for i, df in enumerate([df_wave4, df_wave5, df_wave6, df_wave7], start=4):\n",
    "    print(f\"Wave {i} columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country  year  A124_02_support  A124_06_support  A124_07_support  \\\n",
      "7092     ARG  1999              0.0              0.0              0.0   \n",
      "7093     ARG  1999              0.0              0.0              0.0   \n",
      "7094     ARG  1999              0.0              0.0              1.0   \n",
      "7095     ARG  1999              0.0              0.0              0.0   \n",
      "7096     ARG  1999              0.0              0.0              0.0   \n",
      "\n",
      "      A124_08_support  A124_09_support  C002_support  E036_support  \\\n",
      "7092              0.0              0.0             0             0   \n",
      "7093              1.0              0.0             1             1   \n",
      "7094              1.0              1.0             1             1   \n",
      "7095              0.0              0.0             0             0   \n",
      "7096              0.0              0.0             1             0   \n",
      "\n",
      "      E037_support  ...  G006_oppose  E069_01_oppose  E069_02_oppose  \\\n",
      "7092             1  ...            0               0               1   \n",
      "7093             0  ...            0               0               1   \n",
      "7094             0  ...            0               0               1   \n",
      "7095             0  ...            0               1               1   \n",
      "7096             1  ...            0               1               1   \n",
      "\n",
      "      E069_04_oppose  E069_05_oppose  E069_06_oppose  E069_07_oppose  \\\n",
      "7092               1               1               1               1   \n",
      "7093               1               1               1               1   \n",
      "7094               1               1               1               1   \n",
      "7095               1               1               0               1   \n",
      "7096               0               1               1               1   \n",
      "\n",
      "      E069_08_oppose  E069_13_oppose  E069_17_oppose  \n",
      "7092               1               1               0  \n",
      "7093               1               1               0  \n",
      "7094               1               1               0  \n",
      "7095               1               0               0  \n",
      "7096               1               0               0  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dictionary from JSON file\n",
    "with open(\"variable_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    variable_dict = json.load(file)\n",
    "\n",
    "# Assuming df_encoded is the DataFrame with the survey data after recoding.\n",
    "# Let's inspect df_encoded for LDA\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Este sí jaló\n",
    "\n",
    "# Prepare Data for LDA (Remove Country & Year for now)\n",
    "lda_data = df_encoded.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "# Fit LDA with 10 Ideological Groups\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    doc_topic_prior = 0.25 ,\n",
    "                                    topic_word_prior = 0.1 ,\n",
    "                                    learning_method='online', # online updating not batch faster\n",
    "                                    learning_decay=0.7, # how soon parameters are forgotten\n",
    "                                    learning_offset=10.0, #downweights early learning steppts\n",
    "                                    max_iter=50, # max number of iterations default 10 (iterations in M step)\n",
    "                                    batch_size=1000, #size of batch to use\n",
    "                                    evaluate_every=-1, # evaluate perplexity -1 is off\n",
    "                                    mean_change_tol=0.001, # stopping tolerance for updating in E-step\n",
    "                                    max_doc_update_iter=300, # maximum number of iterations in E-step (iterations over batch)\n",
    "                                    n_jobs=-1, #number of cpu to use\n",
    "                                    random_state=25) #random state, original was in 42 \n",
    "lda_matrix = lda_model.fit_transform(lda_data)\n",
    "\n",
    "# Extract Topic-Feature Importance\n",
    "feature_names = lda_data.columns\n",
    "topic_words = pd.DataFrame(lda_model.components_, columns=feature_names)\n",
    "\n",
    "# Normalize the Importance Scores\n",
    "topic_words = topic_words.div(topic_words.sum(axis=1), axis=0)\n",
    "topic_words = topic_words.T  # Transpose for better visualization\n",
    "topic_words.columns = [f\"Ideology_{i+1}\" for i in range(num_topics)]\n",
    "\n",
    "#### Este sí jaló"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ideology_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideology_5",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "996c5c1b-59a5-4915-95a4-541e86a2ab5a",
       "rows": [
        [
         "0",
         "A124_07_support",
         "F119_oppose",
         "F121_support",
         "F115_support",
         "A124_06_support"
        ],
        [
         "1",
         "A124_09_support",
         "F123_oppose",
         "A124_02_oppose",
         "F114A_support",
         "A124_02_support"
        ],
        [
         "2",
         "A124_08_support",
         "F120_oppose",
         "F117_oppose",
         "F116_support",
         "F119_oppose"
        ],
        [
         "3",
         "F118_oppose",
         "A124_02_oppose",
         "A124_06_oppose",
         "F121_support",
         "F123_oppose"
        ],
        [
         "4",
         "F119_oppose",
         "A124_06_oppose",
         "A124_09_oppose",
         "F120_support",
         "F117_oppose"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ideology_1</th>\n",
       "      <th>Ideology_2</th>\n",
       "      <th>Ideology_3</th>\n",
       "      <th>Ideology_4</th>\n",
       "      <th>Ideology_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A124_07_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F115_support</td>\n",
       "      <td>A124_06_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A124_09_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>F114A_support</td>\n",
       "      <td>A124_02_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A124_08_support</td>\n",
       "      <td>F120_oppose</td>\n",
       "      <td>F117_oppose</td>\n",
       "      <td>F116_support</td>\n",
       "      <td>F119_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F118_oppose</td>\n",
       "      <td>A124_02_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>F121_support</td>\n",
       "      <td>F123_oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F119_oppose</td>\n",
       "      <td>A124_06_oppose</td>\n",
       "      <td>A124_09_oppose</td>\n",
       "      <td>F120_support</td>\n",
       "      <td>F117_oppose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ideology_1      Ideology_2      Ideology_3     Ideology_4  \\\n",
       "0  A124_07_support     F119_oppose    F121_support   F115_support   \n",
       "1  A124_09_support     F123_oppose  A124_02_oppose  F114A_support   \n",
       "2  A124_08_support     F120_oppose     F117_oppose   F116_support   \n",
       "3      F118_oppose  A124_02_oppose  A124_06_oppose   F121_support   \n",
       "4      F119_oppose  A124_06_oppose  A124_09_oppose   F120_support   \n",
       "\n",
       "        Ideology_5  \n",
       "0  A124_06_support  \n",
       "1  A124_02_support  \n",
       "2      F119_oppose  \n",
       "3      F123_oppose  \n",
       "4      F117_oppose  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top Issues for Each Ideology\n",
    "top_issues = topic_words.apply(lambda x: x.nlargest(5).index.tolist(), axis=0)\n",
    "top_issues\n",
    "\n",
    "# Save the descriptive DataFrame to a CSV file\n",
    "top_issues.to_csv('top_issues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Folders for the results\n",
    "\n",
    "# Subfolders exist\n",
    "os.makedirs(\"../reports/lda_evaluation\", exist_ok=True)\n",
    "os.makedirs(\"../reports/top_issues\", exist_ok=True)\n",
    "\n",
    "# Example for saving evaluation results\n",
    "#eval_df.to_csv(f\"reports/lda_evaluation/wave{wave_number}_evaluation.csv\", index=False)\n",
    "\n",
    "# Later when saving top issues:\n",
    "#top_issues_df.to_csv(f\"reports/top_issues/wave{wave_number}_top_issues.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# Setup\n",
    "num_folds = 10\n",
    "topic_range = range(1, 11)  # K = 1 to 10\n",
    "data_folder = \"data\"\n",
    "results_folder = \"../reports/model_evaluation\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "def compute_npmi(issue_list, test_data):\n",
    "    \"\"\"Compute NPMI for a list of top issues using test data.\"\"\"\n",
    "    N = len(test_data)\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    binary_data = test_data[issue_list].astype(int)\n",
    "    issue_counts = binary_data.sum(axis=0).to_dict()\n",
    "\n",
    "    npmi_scores = []\n",
    "    for i, j in combinations(issue_list, 2):\n",
    "        p_i = issue_counts[i] / N\n",
    "        p_j = issue_counts[j] / N\n",
    "        p_ij = ((binary_data[i] & binary_data[j]).sum()) / N\n",
    "\n",
    "        if p_ij > 0:\n",
    "            pmi = np.log(p_ij / (p_i * p_j + epsilon))\n",
    "            npmi = pmi / (-np.log(p_ij + epsilon))\n",
    "            npmi_scores.append(npmi)\n",
    "\n",
    "    return np.mean(npmi_scores) if npmi_scores else 0\n",
    "\n",
    "def evaluate_model_with_npmi(model, test_data, train_data):\n",
    "    \"\"\"Evaluate LDA model using average NPMI across all topics.\"\"\"\n",
    "    feature_names = train_data.columns\n",
    "    topic_words = pd.DataFrame(model.components_, columns=feature_names)\n",
    "    top_issues_per_topic = topic_words.apply(lambda x: x.nlargest(5).index.tolist(), axis=1)\n",
    "\n",
    "    topic_npmis = [compute_npmi(issue_list, test_data) for issue_list in top_issues_per_topic]\n",
    "    return np.mean(topic_npmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌊 Processing Wave 4\n",
      "📂 Fold 1/10\n",
      "📂 Fold 2/10\n",
      "📂 Fold 3/10\n",
      "📂 Fold 4/10\n",
      "📂 Fold 5/10\n",
      "📂 Fold 6/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Run for waves 4–7 sequentially to avoid CPU overload\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m wave_number, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m), [df_wave4, df_wave5, df_wave6, df_wave7]):\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mprocess_wave_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 Done! All waves processed using NPMI evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 37\u001b[0m, in \u001b[0;36mprocess_wave_from_df\u001b[0;34m(wave_number, df_wave)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_idx, n_topics \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topic_range):\n\u001b[1;32m     22\u001b[0m     lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(\n\u001b[1;32m     23\u001b[0m         n_components\u001b[38;5;241m=\u001b[39mn_topics,\n\u001b[1;32m     24\u001b[0m         doc_topic_prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Evaluate using NPMI with held-out test data\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     npmi_score \u001b[38;5;241m=\u001b[39m evaluate_model_with_npmi(lda, test_data, train_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:704\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# calculate final perplexity value on train set\u001b[39;00m\n\u001b[1;32m    701\u001b[0m doc_topics_distr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(\n\u001b[1;32m    702\u001b[0m     X, cal_sstats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, parallel\u001b[38;5;241m=\u001b[39mparallel\n\u001b[1;32m    703\u001b[0m )\n\u001b[0;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perplexity_precomp_distr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topics_distr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    706\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:918\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._perplexity_precomp_distr\u001b[0;34m(self, X, doc_topic_distr, sub_sampling)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of topics does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    917\u001b[0m current_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 918\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_approx_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_distr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sub_sampling:\n\u001b[1;32m    921\u001b[0m     word_cnt \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_samples) \u001b[38;5;241m/\u001b[39m current_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:839\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._approx_bound\u001b[0;34m(self, X, doc_topic_distr, sub_sampling)\u001b[0m\n\u001b[1;32m    835\u001b[0m         cnts \u001b[38;5;241m=\u001b[39m X[idx_d, ids]\n\u001b[1;32m    836\u001b[0m     temp \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    837\u001b[0m         dirichlet_doc_topic[idx_d, :, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m dirichlet_component_[:, ids]\n\u001b[1;32m    838\u001b[0m     )\n\u001b[0;32m--> 839\u001b[0m     norm_phi \u001b[38;5;241m=\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts, norm_phi)\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# compute E[log p(theta | alpha) - log q(theta | gamma)]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/scipy/special/_logsumexp.py:118\u001b[0m, in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp_size(a) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# log of zero is OK\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m         out, sgn \u001b[38;5;241m=\u001b[39m \u001b[43m_logsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# NumPy is convenient for shape manipulation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/scipy/special/_logsumexp.py:218\u001b[0m, in \u001b[0;36m_logsumexp\u001b[0;34m(a, b, axis, return_sign, xp)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Shift, exponentiate, scale, and sum\u001b[39;00m\n\u001b[1;32m    217\u001b[0m exp \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m xp\u001b[38;5;241m.\u001b[39mexp(a \u001b[38;5;241m-\u001b[39m shift) \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mexp(a \u001b[38;5;241m-\u001b[39m shift)\n\u001b[0;32m--> 218\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m s \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mwhere(s \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, s, s\u001b[38;5;241m/\u001b[39mm)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Separate sign/magnitude information\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:2485\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:69\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_wave_from_df(wave_number, df_wave):\n",
    "    print(f\"\\n🌊 Processing Wave {wave_number}\")\n",
    "\n",
    "    # Drop metadata columns so that LDA only uses issue variables\n",
    "    lda_data = df_wave.drop(columns=[\"country\", \"year\"])\n",
    "\n",
    "    # Initialize K-Fold cross-validator\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # To store NPMI evaluation scores across folds and topic numbers\n",
    "    evaluation_matrix = np.zeros((num_folds, len(topic_range)))\n",
    "    npmi_scores_per_topic = {k: [] for k in topic_range}\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(lda_data)):\n",
    "        print(f\"📂 Fold {fold_idx + 1}/{num_folds}\")\n",
    "\n",
    "        train_data = lda_data.iloc[train_idx]\n",
    "        test_data = lda_data.iloc[test_idx]\n",
    "\n",
    "        for k_idx, n_topics in enumerate(topic_range):\n",
    "            lda = LatentDirichletAllocation(\n",
    "                n_components=n_topics,\n",
    "                doc_topic_prior=0.25,\n",
    "                topic_word_prior=0.1,\n",
    "                learning_method='online',\n",
    "                learning_decay=0.7,\n",
    "                learning_offset=10.0,\n",
    "                max_iter=20,\n",
    "                batch_size=1000,\n",
    "                evaluate_every=-1,\n",
    "                mean_change_tol=0.001,\n",
    "                max_doc_update_iter=100,\n",
    "                n_jobs=-1,  # Uses all cores for this model\n",
    "                random_state=25\n",
    "            )\n",
    "            lda.fit(train_data)\n",
    "\n",
    "            # Evaluate using NPMI with held-out test data\n",
    "            npmi_score = evaluate_model_with_npmi(lda, test_data, train_data)\n",
    "            evaluation_matrix[fold_idx, k_idx] = npmi_score\n",
    "            npmi_scores_per_topic[n_topics].append(npmi_score)\n",
    "\n",
    "    # Compute the average NPMI across all folds for each topic count\n",
    "    avg_npmis = {k: np.mean(npmi_scores_per_topic[k]) for k in topic_range}\n",
    "\n",
    "    # Identify the optimal number of topics\n",
    "    optimal_k = max(avg_npmis, key=avg_npmis.get)\n",
    "    print(f\"🏆 Optimal number of topics for Wave {wave_number}: K={optimal_k}\")\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_df = pd.DataFrame(evaluation_matrix, columns=[f\"K={k}\" for k in topic_range])\n",
    "    eval_df.to_csv(f\"{results_folder}/wave{wave_number}_evaluation.csv\", index=False)\n",
    "    print(f\"✅ Saved evaluation results for Wave {wave_number}\")\n",
    "\n",
    "\n",
    "# Run for waves 4–7 sequentially to avoid CPU overload\n",
    "for wave_number, df in zip(range(4, 8), [df_wave4, df_wave5, df_wave6, df_wave7]):\n",
    "    process_wave_from_df(wave_number, df)\n",
    "\n",
    "print(\"\\n🎯 Done! All waves processed using NPMI evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mergin back the results to country and year \n",
    "for wave_df in [df_wave4, df_wave5, df_wave6, df_wave7]:\n",
    "    lda_data = wave_df.drop(columns=[\"country\", \"year\"])\n",
    "    lda.fit(lda_data)\n",
    "    topic_dist = lda.transform(lda_data)\n",
    "    topic_df = pd.DataFrame(topic_dist, columns=[f\"Topic_{i+1}\" for i in range(topic_dist.shape[1])])\n",
    "    merged = pd.concat([wave_df.reset_index(drop=True), topic_df], axis=1)\n",
    "\n",
    "    # Optionally save it\n",
    "    year_range = f\"{wave_df['year'].min()}_{wave_df['year'].max()}\"\n",
    "    merged.to_csv(f\"../reports/merged_wave_{year_range}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
